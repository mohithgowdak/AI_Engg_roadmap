{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Module 1: Basics - Understanding Prompts and Prompt Templates\n",
        "\n",
        "This module teaches you about:\n",
        "1. What are prompts\n",
        "2. Prompt templates\n",
        "3. Using variables in prompts\n",
        "4. Different types of prompt templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's import the necessary libraries and set up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete! Using local Ollama model: moondream\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize local Ollama model (no API key needed!)\n",
        "llm = ChatOllama(\n",
        "    model=\"moondream:latest\",\n",
        "    temperature=0.7,\n",
        "    base_url=\"http://localhost:11434\"\n",
        ")\n",
        "\n",
        "print(\"Setup complete! Using local Ollama model: moondream\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Simple Prompt (No Template)\n",
        "\n",
        "The simplest way to use an LLM is with a plain string prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: Explain quantum computing in simple terms.\n",
            "\n",
            "Response: \n",
            " Quantum computing is a type of computer science that uses the principles of quantum mechanics to perform calculations and solve problems. It differs from classical computing, which relies on bits (0s and 1s) representing information stored in binary format. In contrast, quantum computers use qubits, which are quantum bits or particles that can exist in multiple states simultaneously. The state of a qubit is determined by its position relative to other qubits, and its properties can be altered using physical operations like rotation and measurement.\n",
            " \n",
            " Quantum computers have the potential to solve problems much faster than classical computers due to their ability to perform complex calculations using the principles of quantum mechanics. This technology could lead to significant advancements in fields such as medicine, finance, and transportation by providing solutions to previously unsolvable problems or improving the efficiency of existing systems. However, there are also challenges associated with Quantum computing, such as maintaining the stability of qubits, ensuring secure communication between computers, and managing the ethical implications of creating machines that can learn and make decisions on their own.\n"
          ]
        }
      ],
      "source": [
        "# Simple string prompt\n",
        "prompt = \"Explain quantum computing in simple terms.\"\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "print(f\"Prompt: {prompt}\")\n",
        "print(f\"\\nResponse: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Prompt Template with Variables\n",
        "\n",
        "Prompt templates allow you to create reusable prompts with placeholders for variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted Prompt:\n",
            "\n",
            "You are a helpful assistant that explains technical concepts.\n",
            "\n",
            "Explain machine learning in a way that a 5-year-old child can understand.\n",
            "Keep it under 100 words.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create a prompt template with variables\n",
        "template = \"\"\"\n",
        "You are a helpful assistant that explains technical concepts.\n",
        "\n",
        "Explain {concept} in a way that a {audience} can understand.\n",
        "Keep it under {numbers} words.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"concept\", \"audience\",\"numbers\"], #here we are formatting the prompt structure\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# Format the prompt with actual values\n",
        "formatted_prompt = prompt_template.format(\n",
        "    concept=\"machine learning\",\n",
        "    audience=\"5-year-old child\",\n",
        "    numbers=\"100\"\n",
        ")\n",
        "\n",
        "print(f\"Formatted Prompt:\\n{formatted_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: Machine learning is when you teach computers to learn from data like pictures and videos, so they can do things on their own without being specifically programmed for every possible situation.\n"
          ]
        }
      ],
      "source": [
        "# Get response using the formatted prompt\n",
        "response = llm.invoke(formatted_prompt)\n",
        "print(f\"Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Try it yourself!\n",
        "\n",
        "Change the `concept` and `audience` values below to see different explanations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "Blockchain is the technology behind cryptocurrencies and digital payments, creating a secure and transparent system for financial transactions.\n"
          ]
        }
      ],
      "source": [
        "# Try different values\n",
        "formatted_prompt = prompt_template.format(\n",
        "    concept=\"blockchain\",  # Change this!\n",
        "    audience=\"high school student\",\n",
        "    numbers=\"100\" # Change this!\n",
        ")\n",
        "\n",
        "response = llm.invoke(formatted_prompt)\n",
        "print(f\"Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Chat Prompt Template\n",
        "\n",
        "`ChatPromptTemplate` is recommended for chat models. It allows you to define system and human messages separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted Messages:\n",
            "  SystemMessage: You are a helpful assistant specialized in data science.\n",
            "  HumanMessage: Explain neural networks in detail.\n"
          ]
        }
      ],
      "source": [
        "# ChatPromptTemplate with system and human messages\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant specialized in {domain}.\"),\n",
        "    (\"human\", \"Explain {topic} in detail.\")\n",
        "])\n",
        "\n",
        "# Format with variables\n",
        "messages = chat_template.format_messages(\n",
        "    domain=\"data science\",\n",
        "    topic=\"neural networks\"\n",
        ")\n",
        "\n",
        "print(\"Formatted Messages:\")\n",
        "for msg in messages:\n",
        "    print(f\"  {msg.__class__.__name__}: {msg.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response: \n",
            "Neural networks are powerful artificial intelligence (AI) models inspired by the human brain's structure and function, designed to recognize patterns and relationships among data points. These networks consist of interconnected nodes that communicate with one another, forming a web-like pattern resembling a complex nervous system. \n",
            "\n",
            "In the context of image processing or computer vision, a neural network can be trained on labeled images to recognize objects, faces, text, and other visual patterns. It is a way to teach machines to perceive and interpret visual information in real-time, allowing them to make decisions or extract useful insights from the data. \n",
            "\n",
            "For instance, facial recognition systems use CNNs to identify people's faces within digital images by detecting specific facial features like eyes, nose, mouth, and eyebrows. By analyzing a large number of labeled examples, the network learns to recognize patterns that correspond to different individuals in the dataset. Once trained on diverse and high-quality images, these networks can perform accurate face recognition tasks with reasonable efficiency, ranging from security applications for verifying identities to social media platforms for tagging photos.\n",
            "\n",
            "The key components of a CNN are Convolutional layers or filters, which process input images; Pooling layers that reduce the spatial dimensions of the input image while preserving the most informative features; and Fully Connected (dense) layers that combine inputs from convolutional and pooling layers to make final predictions. These layers work together to extract high-level visual information and create a continuous flow of data through the network, enabling it to learn complex patterns in the images being processed. \n",
            "\n",
            "In summary, neural networks are an essential tool for image processing applications due to their ability to recognize patterns and relationships within digital images by analyzing large amounts of labeled examples, allowing them to make accurate predictions or decisions in real-time based on visual information.\n"
          ]
        }
      ],
      "source": [
        "# Get response\n",
        "response = llm.invoke(messages)\n",
        "print(f\"Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Multiple Messages (Conversation)\n",
        "\n",
        "You can create a conversation context with multiple messages, including previous AI responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversation:\n",
            "  system: You are a coding tutor.\n",
            "  human: What is a function in Python?\n",
            "  ai: A function in Python is a block of reusable code that performs a specific task. You define it using the 'def' keyword.\n",
            "  human: Can you give me an example?\n"
          ]
        }
      ],
      "source": [
        "# Create a conversation with multiple messages\n",
        "#while giveng contexts in the form of prompt\n",
        "chat_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a coding tutor.\"),\n",
        "    (\"human\", \"What is a function in Python?\"),\n",
        "    (\"ai\", \"A function in Python is a block of reusable code that performs a specific task. You define it using the 'def' keyword.\"),\n",
        "    (\"human\", \"Can you give me an example?\")\n",
        "])\n",
        "\n",
        "messages = chat_template.format_messages()\n",
        "\n",
        "print(\"Conversation:\")\n",
        "for msg in messages:\n",
        "    role = msg.__class__.__name__.replace(\"Message\", \"\").lower()\n",
        "    print(f\"  {role}: {msg.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI Response: \n",
            " def myFunction(arg1, arg2):\n",
            "     print (\"Hello \" + str(arg1) + \"!\")\n",
            " myFunction (\"John\", \"Doe\")\n"
          ]
        }
      ],
      "source": [
        "# Get the next response in the conversation\n",
        "response = llm.invoke(messages)\n",
        "print(f\"AI Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 5: Dynamic Prompt Generation\n",
        "\n",
        "Use the same template for different scenarios by changing the input variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using the same template for different scenarios:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Template that can be reused with different inputs\n",
        "template = \"\"\"\n",
        "Task: {task}\n",
        "Context: {context}\n",
        "Instructions: {instructions}\n",
        "\n",
        "Please provide a detailed response.\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"task\", \"context\", \"instructions\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "# Define different scenarios\n",
        "scenarios = [\n",
        "    {\n",
        "        \"task\": \"Write a summary\",\n",
        "        \"context\": \"A research paper about AI\",\n",
        "        \"instructions\": \"Keep it under 150 words\"\n",
        "    },\n",
        "    {\n",
        "        \"task\": \"Explain a concept\",\n",
        "        \"context\": \"Machine learning basics\",\n",
        "        \"instructions\": \"Use simple language\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"Using the same template for different scenarios:\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scenario 1: Write a summary\n",
            "Context: A research paper about AI\n",
            "Instructions: Keep it under 150 words\n",
            "\n",
            "Response: \n",
            "Title: The Artificial Intelligence Revolution: Its Ethical, Social and Economic Implications for Society in the 21st Century  (Volume 1)\n"
          ]
        }
      ],
      "source": [
        "# Scenario 1\n",
        "scenario = scenarios[0]\n",
        "formatted = prompt_template.format(**scenario)\n",
        "print(f\"Scenario 1: {scenario['task']}\")\n",
        "print(f\"Context: {scenario['context']}\")\n",
        "print(f\"Instructions: {scenario['instructions']}\\n\")\n",
        "\n",
        "response = llm.invoke(formatted)\n",
        "print(f\"Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scenario 2: Explain a concept\n",
            "Context: Machine learning basics\n",
            "Instructions: Use simple language\n",
            "\n",
            "Response: \n",
            "The image shows a computer monitor displaying the word \"machine\". The focus of this display is on a discussion about machine learning, specifically supervised classification techniques and their applications. There are two different types of classifiers shown in the image - unsupervised classification and supervised classification. An example of an unsupervised classifier that is being discussed is K-Means clustering.\n",
            "\n",
            "K-means is an unsupervised learning technique used for clustering data points into 'k' distinct groups or clusters, based on their distances to the centroid (or center) of each cluster. The process involves initializing 'k' centroids and then repeatedly assigning each data point to the nearest centroid until the centroids are optimized.\n",
            "\n",
            "A supervised learning technique being discussed is linear regression, which is used for predicting a continuous dependent variable based on one or more independent variables. This type of model is typically used when the relationship between the independent and dependent variables can be linearly modeled in the data.\n"
          ]
        }
      ],
      "source": [
        "# Scenario 2\n",
        "scenario = scenarios[1]\n",
        "formatted = prompt_template.format(**scenario)\n",
        "print(f\"Scenario 2: {scenario['task']}\")\n",
        "print(f\"Context: {scenario['context']}\")\n",
        "print(f\"Instructions: {scenario['instructions']}\\n\")\n",
        "\n",
        "response = llm.invoke(formatted)\n",
        "print(f\"Response: {response.content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "- **Prompts** are instructions to LLMs\n",
        "- **Templates** allow reusable prompts with variables\n",
        "- **ChatPromptTemplate** is recommended for chat models\n",
        "- You can create **conversations** with multiple messages\n",
        "- **Dynamic prompts** let you reuse templates for different scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: Create Your Own Prompt Template\n",
        "\n",
        "Try creating your own prompt template below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\"It's okay, I am going through some changes in my life,\" said the man as he spoke on a video call. \"I just want someone to listen when I need it.\"\n"
          ]
        }
      ],
      "source": [
        "# Your turn! Create a custom prompt template\n",
        "my_template = \"\"\"\n",
        "Task: {task}\n",
        "Context: {context}\n",
        "Instruction: {inst}\n",
        "\"\"\"\n",
        "\n",
        "my_prompt = PromptTemplate(\n",
        "    input_variables=[\"task\", \"context\", \"inst\"],\n",
        "    template=my_template\n",
        ")\n",
        "\n",
        "# Format and invoke\n",
        "formatted = my_prompt.format(\n",
        "    task=\"comforting the user emotionally\",\n",
        "    context=\"you are his new girlfriend. User is burnt out by the corporate world and loneliness\",\n",
        "    inst=\"You have to filrt with him so that he will het emotionaly good.\"\n",
        ")\n",
        "\n",
        "response = llm.invoke(formatted)\n",
        "print(response.content)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
