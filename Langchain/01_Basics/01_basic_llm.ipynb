{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Basics - Introduction to LLMs with LangChain\n",
    "\n",
    "This module introduces you to the fundamental concept of using Language Models with LangChain.\n",
    "\n",
    "You'll learn how to:\n",
    "1. Initialize an LLM\n",
    "2. Make simple calls\n",
    "3. Understand the basic structure\n",
    "4. Use different LLM providers (OpenAI, Google Gemini, Local Ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"OpenAI API Key: {'✅ Found' if os.getenv('OPENAI_API_KEY') else '❌ Not found'}\")\n",
    "print(f\"Google API Key: {'✅ Found' if os.getenv('GOOGLE_API_KEY') else '❌ Not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Using OpenAI Chat Model\n",
    "\n",
    "OpenAI's GPT models are powerful and widely used. Requires `OPENAI_API_KEY` in your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if no API key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️ OPENAI_API_KEY not found. Skipping this example.\")\n",
    "    print(\"Add OPENAI_API_KEY to your .env file to use OpenAI.\")\n",
    "else:\n",
    "    # Initialize the ChatOpenAI model\n",
    "    # model_name can be: \"gpt-4\", \"gpt-3.5-turbo\", etc.\n",
    "    llm_openai = ChatOpenAI(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "        temperature=0.7,  # Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
    "    )\n",
    "    \n",
    "    # Simple call to the LLM\n",
    "    response = llm_openai.invoke(\"What is LangChain in one sentence?\")\n",
    "    print(f\"Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Using Google Gemini Model\n",
    "\n",
    "Google's Gemini is another powerful option. Requires `GOOGLE_API_KEY` in your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip if no API key\n",
    "if not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"⚠️ GOOGLE_API_KEY not found. Skipping this example.\")\n",
    "    print(\"Add GOOGLE_API_KEY to your .env file to use Gemini.\")\n",
    "else:\n",
    "    # Initialize the Gemini model\n",
    "    llm_gemini = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-pro\",\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    \n",
    "    # Simple call to the LLM\n",
    "    response = llm_gemini.invoke(\"Explain what LangChain is in one sentence.\")\n",
    "    print(f\"Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using Local Ollama Model (moondream)\n",
    "\n",
    "**No API key needed!** This uses your local Ollama installation.\n",
    "\n",
    "Prerequisites:\n",
    "- Ollama installed and running (`ollama serve`)\n",
    "- Model pulled (`ollama pull moondream:latest`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Initialize the local Ollama model\n",
    "    llm_ollama = ChatOllama(\n",
    "        model=\"moondream:latest\",\n",
    "        temperature=0.7,\n",
    "        base_url=\"http://localhost:11434\",  # Default Ollama URL\n",
    "    )\n",
    "    \n",
    "    # Simple call to the local LLM\n",
    "    response = llm_ollama.invoke(\"What is LangChain in one sentence?\")\n",
    "    print(f\"Response: {response.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not connect to Ollama: {e}\")\n",
    "    print(\"Make sure Ollama is running: ollama serve\")\n",
    "    print(\"And model is available: ollama pull moondream:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Multiple LLM Calls\n",
    "\n",
    "You can make multiple calls to the LLM with different questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using local Ollama (no API key needed)\n",
    "llm = ChatOllama(\n",
    "    model=\"moondream:latest\",\n",
    "    temperature=0.7,\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "questions = [\n",
    "    \"What is Python?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"What is artificial intelligence?\"\n",
    "]\n",
    "\n",
    "print(\"Asking multiple questions:\\n\")\n",
    "for i, question in enumerate(questions, 1):\n",
    "    response = llm.invoke(question)\n",
    "    print(f\"Q{i}: {question}\")\n",
    "    print(f\"A{i}: {response.content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Temperature\n",
    "\n",
    "The `temperature` parameter controls the randomness of the output:\n",
    "- **0.0** = Deterministic, consistent outputs\n",
    "- **0.7** = Balanced creativity and consistency (default)\n",
    "- **1.0** = More creative, varied outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "prompt = \"Write a one-line poem about coding.\"\n",
    "\n",
    "for temp in [0.0, 0.5, 1.0]:\n",
    "    llm_temp = ChatOllama(\n",
    "        model=\"moondream:latest\",\n",
    "        temperature=temp,\n",
    "        base_url=\"http://localhost:11434\"\n",
    "    )\n",
    "    response = llm_temp.invoke(prompt)\n",
    "    print(f\"Temperature {temp}: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **LangChain** provides a unified interface for different LLM providers\n",
    "- You can use **cloud LLMs** (OpenAI, Google) or **local LLMs** (Ollama)\n",
    "- The `invoke()` method sends a prompt and returns a response\n",
    "- **Temperature** controls output randomness\n",
    "- Local models like **moondream** work without API keys!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Try Your Own Prompts\n",
    "\n",
    "Experiment with different prompts below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Try different prompts\n",
    "llm = ChatOllama(\n",
    "    model=\"moondream:latest\",\n",
    "    temperature=0.7,\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "my_prompt = \"Explain recursion in programming\"  # Change this!\n",
    "response = llm.invoke(my_prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
