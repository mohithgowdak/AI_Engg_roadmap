{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Basics - Creating and Using Chains\n",
    "\n",
    "Chains are sequences of calls to LLMs or other utilities.\n",
    "\n",
    "This module covers:\n",
    "1. Simple chains (LLM + Prompt)\n",
    "2. Sequential chains\n",
    "3. Custom chains\n",
    "4. Batch processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize local Ollama model (no API key needed!)\n",
    "llm = ChatOllama(\n",
    "    model=\"moondream:latest\",\n",
    "    temperature=0.7,\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "print(\"Setup complete! Using local Ollama model: moondream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Simple Chain (LLM + Prompt)\n",
    "\n",
    "The simplest chain combines a prompt template with an LLM using the `|` (pipe) operator.\n",
    "\n",
    "This is the modern **LCEL (LangChain Expression Language)** way to create chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in simple terms.\"\n",
    ")\n",
    "\n",
    "# Create a chain using the pipe operator (LCEL)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run the chain\n",
    "response = chain.invoke({\"topic\": \"blockchain\"})\n",
    "\n",
    "print(f\"Topic: blockchain\")\n",
    "print(f\"\\nResponse: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try different topics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with a different topic\n",
    "topics = [\"quantum computing\", \"neural networks\", \"cloud computing\"]\n",
    "\n",
    "for topic in topics:\n",
    "    response = chain.invoke({\"topic\": topic})\n",
    "    print(f\"\\nüìå {topic.upper()}\")\n",
    "    print(f\"{response[:200]}...\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Sequential Chain\n",
    "\n",
    "Sequential chains pass the output of one step to the next. Let's create a chain that:\n",
    "1. Generates a story\n",
    "2. Summarizes the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First chain: Generate a story\n",
    "story_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a very short story (2-3 sentences) about {topic}.\"\n",
    ")\n",
    "story_chain = story_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Second chain: Summarize the story\n",
    "summary_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Summarize this story in one sentence: {story}\"\n",
    ")\n",
    "summary_chain = summary_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Combine them: story output becomes summary input\n",
    "full_chain = (\n",
    "    {\"story\": story_chain}\n",
    "    | summary_chain\n",
    ")\n",
    "\n",
    "# Run the combined chain\n",
    "topic = \"a robot learning to paint\"\n",
    "story = story_chain.invoke({\"topic\": topic})\n",
    "summary = full_chain.invoke({\"topic\": topic})\n",
    "\n",
    "print(f\"Topic: {topic}\")\n",
    "print(f\"\\nüìñ Story:\\n{story}\")\n",
    "print(f\"\\nüìù Summary:\\n{summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Chain with Multiple Variables\n",
    "\n",
    "Chains can handle multiple input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain that uses multiple variables\n",
    "multi_var_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that formats information clearly.\"),\n",
    "    (\"human\", \"\"\"\n",
    "    Topic: {topic}\n",
    "    Format: {format}\n",
    "    \n",
    "    Provide information about the topic in the requested format.\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "format_chain = multi_var_prompt | llm | StrOutputParser()\n",
    "\n",
    "result = format_chain.invoke({\n",
    "    \"topic\": \"Python programming\",\n",
    "    \"format\": \"3 bullet points\"\n",
    "})\n",
    "\n",
    "print(\"Topic: Python programming\")\n",
    "print(\"Format: 3 bullet points\")\n",
    "print(f\"\\nResponse:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Chain with RunnablePassthrough\n",
    "\n",
    "`RunnablePassthrough` allows you to pass data through unchanged while also running other operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain that keeps the original input and adds processed data\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is {concept}? Explain briefly.\"\n",
    ")\n",
    "\n",
    "chain_with_passthrough = (\n",
    "    {\"concept\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Now you can pass the concept directly as a string\n",
    "result = chain_with_passthrough.invoke(\"machine learning\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Batch Processing\n",
    "\n",
    "Process multiple inputs at once using the `batch()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple explanation chain\n",
    "explain_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {concept} in one sentence.\"\n",
    ")\n",
    "\n",
    "explain_chain = explain_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Process multiple concepts at once\n",
    "concepts = [\"machine learning\", \"neural networks\", \"deep learning\"]\n",
    "inputs = [{\"concept\": c} for c in concepts]\n",
    "\n",
    "# Batch process all inputs\n",
    "results = explain_chain.batch(inputs)\n",
    "\n",
    "print(\"Batch processing results:\\n\")\n",
    "for concept, result in zip(concepts, results):\n",
    "    print(f\"üìå {concept}:\")\n",
    "    print(f\"   {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 6: Streaming Output\n",
    "\n",
    "For long responses, you can stream the output token by token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for longer content\n",
    "story_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a short paragraph about {topic}.\"\n",
    ")\n",
    "\n",
    "stream_chain = story_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Stream the output\n",
    "print(\"Streaming response:\\n\")\n",
    "for chunk in stream_chain.stream({\"topic\": \"the future of AI\"}):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print(\"\\n\\n‚úÖ Stream complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "- **Chains** combine prompts and LLMs into reusable pipelines\n",
    "- Use the **pipe operator** `|` for LCEL (LangChain Expression Language)\n",
    "- **Sequential chains** pass output from one step to the next\n",
    "- **batch()** processes multiple inputs efficiently\n",
    "- **stream()** shows output as it's generated\n",
    "- **RunnablePassthrough** passes data through unchanged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Build Your Own Chain\n",
    "\n",
    "Create a chain that:\n",
    "1. Takes a topic\n",
    "2. Generates 3 questions about it\n",
    "3. Answers one of those questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! Build a custom chain\n",
    "\n",
    "# Step 1: Generate questions\n",
    "question_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Generate 3 interesting questions about {topic}. List them numbered.\"\n",
    ")\n",
    "question_chain = question_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 2: Answer the first question\n",
    "answer_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Here are some questions:\\n{questions}\\n\\nAnswer the first question briefly.\"\n",
    ")\n",
    "answer_chain = answer_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Combine them\n",
    "full_chain = (\n",
    "    {\"questions\": question_chain}\n",
    "    | answer_chain\n",
    ")\n",
    "\n",
    "# Test it\n",
    "topic = \"space exploration\"  # Change this!\n",
    "questions = question_chain.invoke({\"topic\": topic})\n",
    "answer = full_chain.invoke({\"topic\": topic})\n",
    "\n",
    "print(f\"Topic: {topic}\\n\")\n",
    "print(f\"Questions:\\n{questions}\\n\")\n",
    "print(f\"Answer to first question:\\n{answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
